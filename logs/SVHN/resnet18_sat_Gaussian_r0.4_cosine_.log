Namespace(arch='resnet18', base_width=64, batch_size=128, data_root='../DATASETS/SVHN', dataset='SVHN', epochs=100, evaluate=False, loss='sat', lr=0.1, lr_gamma=0.1, lr_milestones=[40, 80], lr_schedule='cosine', momentum=0.9, noise_info=None, noise_rate=0.4, noise_type='Gaussian', optimizer='sgd', print_freq=50, result_dir='results/SVHN/resnet18_sat_Gaussian_r0.4_cosine_', resume='', sat_alpha=0.9, sat_es=60, save_dir='ckpts/SVHN/resnet18_sat_Gaussian_r0.4_cosine_', save_freq=0, seed=497, start_epoch=0, train_sets='trainval', turn_off_aug=False, use_refined_label=False, val_sets=['test_set'], weight_decay=0.0005, workers=4)
Using downloaded and verified file: ../DATASETS/SVHN/train_32x32.mat
data shape: (73257, 3, 32, 32)
Using downloaded and verified file: ../DATASETS/SVHN/test_32x32.mat
data shape: (26032, 3, 32, 32)
Randomizing 40.0 percent of images with `Gaussian` scheme.
n_train:73257  n_rand:29302 randomize_indices:[58902 28198 43988 ... 38961 42572 34405]
randomize_iamges: (29302, 32, 32, 3)
randomize_iamges: (29302, 3, 32, 32)
Noise info is saved to ckpts/SVHN/resnet18_sat_Gaussian_r0.4_cosine_
Size of dataset: 73257.
Using `SGD` optimizer
Using `cosine` schedule
****************************************
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch: [0][50/573]	LR 0.100000	Time 0.400 (0.499)	Data 0.000 (0.016)	Loss 2.2245 (2.9158)	Prec@1 23.438 (17.344)
Epoch: [0][100/573]	LR 0.100000	Time 0.402 (0.449)	Data 0.000 (0.008)	Loss 2.2354 (2.5824)	Prec@1 19.531 (17.734)
Epoch: [0][150/573]	LR 0.100000	Time 0.404 (0.433)	Data 0.000 (0.005)	Loss 2.2012 (2.4702)	Prec@1 25.781 (18.193)
Epoch: [0][200/573]	LR 0.100000	Time 0.400 (0.423)	Data 0.000 (0.004)	Loss 2.2513 (2.4123)	Prec@1 15.625 (18.219)
Epoch: [0][250/573]	LR 0.100000	Time 0.375 (0.418)	Data 0.000 (0.003)	Loss 2.2223 (2.3778)	Prec@1 20.312 (18.369)
Epoch: [0][300/573]	LR 0.100000	Time 0.403 (0.415)	Data 0.001 (0.003)	Loss 2.1987 (2.3541)	Prec@1 21.094 (18.602)
Epoch: [0][350/573]	LR 0.100000	Time 0.404 (0.414)	Data 0.000 (0.003)	Loss 2.2741 (2.3373)	Prec@1 14.062 (18.603)
Epoch: [0][400/573]	LR 0.100000	Time 0.402 (0.412)	Data 0.001 (0.002)	Loss 2.2396 (2.3253)	Prec@1 22.656 (18.633)
Epoch: [0][450/573]	LR 0.100000	Time 0.372 (0.410)	Data 0.000 (0.002)	Loss 2.2409 (2.3162)	Prec@1 15.625 (18.590)
Epoch: [0][500/573]	LR 0.100000	Time 0.402 (0.409)	Data 0.000 (0.002)	Loss 2.2556 (2.3082)	Prec@1 17.188 (18.670)
Epoch: [0][550/573]	LR 0.100000	Time 0.402 (0.408)	Data 0.000 (0.002)	Loss 2.2190 (2.3019)	Prec@1 21.875 (18.666)
Epoch: [0][573/573]	LR 0.100000	Time 3.441 (0.413)	Data 0.000 (0.002)	Loss 2.2652 (2.2993)	Prec@1 17.073 (18.667)
****************************************
test_set:	 * Prec@1 19.576
****************************************
Epoch: [1][50/573]	LR 0.099975	Time 0.365 (0.405)	Data 0.000 (0.005)	Loss 2.2781 (2.2441)	Prec@1 16.406 (18.531)
Epoch: [1][100/573]	LR 0.099975	Time 0.403 (0.402)	Data 0.000 (0.003)	Loss 2.2504 (2.2419)	Prec@1 21.094 (19.039)
Epoch: [1][150/573]	LR 0.099975	Time 0.403 (0.402)	Data 0.000 (0.002)	Loss 2.2165 (2.2394)	Prec@1 17.969 (18.844)
Epoch: [1][200/573]	LR 0.099975	Time 0.401 (0.402)	Data 0.000 (0.001)	Loss 2.2077 (2.2389)	Prec@1 22.656 (18.953)
Epoch: [1][250/573]	LR 0.099975	Time 0.403 (0.401)	Data 0.000 (0.001)	Loss 2.2152 (2.2378)	Prec@1 21.875 (19.047)
Epoch: [1][300/573]	LR 0.099975	Time 0.403 (0.401)	Data 0.000 (0.001)	Loss 2.2332 (2.2367)	Prec@1 18.750 (19.042)
Epoch: [1][350/573]	LR 0.099975	Time 0.402 (0.402)	Data 0.000 (0.001)	Loss 2.1994 (2.2362)	Prec@1 23.438 (19.076)
Epoch: [1][400/573]	LR 0.099975	Time 0.401 (0.402)	Data 0.000 (0.001)	Loss 2.2094 (2.2367)	Prec@1 17.969 (19.061)
Epoch: [1][450/573]	LR 0.099975	Time 0.401 (0.402)	Data 0.000 (0.001)	Loss 2.2402 (2.2368)	Prec@1 18.750 (18.988)
Epoch: [1][500/573]	LR 0.099975	Time 0.384 (0.402)	Data 0.000 (0.001)	Loss 2.2640 (2.2367)	Prec@1 17.969 (18.964)
Epoch: [1][550/573]	LR 0.099975	Time 0.399 (0.401)	Data 0.000 (0.001)	Loss 2.2501 (2.2371)	Prec@1 16.406 (18.926)
Epoch: [1][573/573]	LR 0.099975	Time 0.182 (0.401)	Data 0.000 (0.001)	Loss 2.2019 (2.2373)	Prec@1 19.512 (18.823)
****************************************
test_set:	 * Prec@1 19.587
****************************************
Epoch: [2][50/573]	LR 0.099901	Time 0.395 (0.405)	Data 0.000 (0.005)	Loss 2.2431 (2.2343)	Prec@1 17.188 (19.203)
Epoch: [2][100/573]	LR 0.099901	Time 0.403 (0.403)	Data 0.000 (0.003)	Loss 2.2587 (2.2358)	Prec@1 14.062 (19.008)
Epoch: [2][150/573]	LR 0.099901	Time 0.404 (0.403)	Data 0.000 (0.002)	Loss 2.2275 (2.2338)	Prec@1 22.656 (19.135)
Epoch: [2][200/573]	LR 0.099901	Time 0.400 (0.402)	Data 0.000 (0.001)	Loss 2.2971 (2.2354)	Prec@1 6.250 (18.934)
Epoch: [2][250/573]	LR 0.099901	Time 0.404 (0.401)	Data 0.000 (0.001)	Loss 2.2191 (2.2359)	Prec@1 21.875 (18.931)
Epoch: [2][300/573]	LR 0.099901	Time 0.404 (0.402)	Data 0.000 (0.001)	Loss 2.2363 (2.2360)	Prec@1 20.312 (18.865)
Epoch: [2][350/573]	LR 0.099901	Time 0.404 (0.402)	Data 0.000 (0.001)	Loss 2.1996 (2.2357)	Prec@1 18.750 (18.897)
Epoch: [2][400/573]	LR 0.099901	Time 0.402 (0.402)	Data 0.000 (0.001)	Loss 2.2477 (2.2370)	Prec@1 14.844 (18.816)
Epoch: [2][450/573]	LR 0.099901	Time 0.401 (0.402)	Data 0.000 (0.001)	Loss 2.2003 (2.2354)	Prec@1 21.875 (19.007)
Epoch: [2][500/573]	LR 0.099901	Time 0.377 (0.402)	Data 0.000 (0.001)	Loss 2.1768 (2.2354)	Prec@1 20.312 (19.031)
Epoch: [2][550/573]	LR 0.099901	Time 0.399 (0.402)	Data 0.000 (0.001)	Loss 2.2958 (2.2341)	Prec@1 19.531 (19.129)
Epoch: [2][573/573]	LR 0.099901	Time 0.182 (0.401)	Data 0.000 (0.001)	Loss 2.2118 (2.2341)	Prec@1 17.073 (19.077)
****************************************
test_set:	 * Prec@1 19.849
****************************************
Epoch: [3][50/573]	LR 0.099778	Time 0.372 (0.406)	Data 0.000 (0.006)	Loss 2.2455 (2.2162)	Prec@1 17.969 (20.797)
Epoch: [3][100/573]	LR 0.099778	Time 0.403 (0.403)	Data 0.000 (0.003)	Loss 2.1412 (2.2120)	Prec@1 28.906 (20.969)
Epoch: [3][150/573]	LR 0.099778	Time 0.404 (0.403)	Data 0.000 (0.002)	Loss 2.2337 (2.2112)	Prec@1 22.656 (20.964)
Epoch: [3][200/573]	LR 0.099778	Time 0.400 (0.402)	Data 0.000 (0.002)	Loss 2.2502 (2.2061)	Prec@1 14.844 (21.258)
Epoch: [3][250/573]	LR 0.099778	Time 0.404 (0.402)	Data 0.000 (0.001)	Loss 2.0707 (2.1947)	Prec@1 26.562 (21.603)
Epoch: [3][300/573]	LR 0.099778	Time 0.404 (0.402)	Data 0.000 (0.001)	Loss 1.9989 (2.1773)	Prec@1 28.125 (22.206)
Epoch: [3][350/573]	LR 0.099778	Time 0.404 (0.402)	Data 0.000 (0.001)	Loss 1.9783 (2.1530)	Prec@1 35.156 (22.933)
Epoch: [3][400/573]	LR 0.099778	Time 0.402 (0.402)	Data 0.000 (0.001)	Loss 1.7954 (2.1180)	Prec@1 33.594 (24.236)
Epoch: [3][450/573]	LR 0.099778	Time 0.401 (0.402)	Data 0.000 (0.001)	Loss 1.6371 (2.0773)	Prec@1 37.500 (25.731)
Epoch: [3][500/573]	LR 0.099778	Time 0.395 (0.402)	Data 0.000 (0.001)	Loss 1.6109 (2.0312)	Prec@1 42.969 (27.480)
Epoch: [3][550/573]	LR 0.099778	Time 0.400 (0.402)	Data 0.000 (0.001)	Loss 1.3499 (1.9854)	Prec@1 50.781 (29.124)
Epoch: [3][573/573]	LR 0.099778	Time 0.180 (0.401)	Data 0.000 (0.001)	Loss 1.5212 (1.9649)	Prec@1 46.341 (29.861)
****************************************
test_set:	 * Prec@1 56.108
****************************************
Epoch: [4][50/573]	LR 0.099606	Time 0.357 (0.404)	Data 0.000 (0.005)	Loss 1.2701 (1.3895)	Prec@1 54.688 (51.031)
Epoch: [4][100/573]	LR 0.099606	Time 0.404 (0.402)	Data 0.000 (0.002)	Loss 1.2934 (1.3715)	Prec@1 59.375 (51.438)
Epoch: [4][150/573]	LR 0.099606	Time 0.404 (0.403)	Data 0.000 (0.002)	Loss 1.2809 (1.3615)	Prec@1 51.562 (51.969)
Epoch: [4][200/573]	LR 0.099606	Time 0.400 (0.402)	Data 0.000 (0.001)	Loss 1.1503 (1.3418)	Prec@1 60.938 (52.801)
Epoch: [4][250/573]	LR 0.099606	Time 0.404 (0.401)	Data 0.000 (0.001)	Loss 1.3036 (1.3291)	Prec@1 54.688 (53.178)
Epoch: [4][300/573]	LR 0.099606	Time 0.404 (0.402)	Data 0.000 (0.001)	Loss 1.2017 (1.3191)	Prec@1 58.594 (53.555)
Epoch: [4][350/573]	LR 0.099606	Time 0.404 (0.402)	Data 0.000 (0.001)	Loss 1.2160 (1.3062)	Prec@1 53.906 (54.031)
Epoch: [4][400/573]	LR 0.099606	Time 0.401 (0.402)	Data 0.000 (0.001)	Loss 1.1451 (1.2951)	Prec@1 60.156 (54.461)
Epoch: [4][450/573]	LR 0.099606	Time 0.403 (0.402)	Data 0.000 (0.001)	Loss 1.2855 (1.2850)	Prec@1 57.031 (54.795)
Epoch: [4][500/573]	LR 0.099606	Time 0.401 (0.402)	Data 0.000 (0.001)	Loss 1.3652 (1.2789)	Prec@1 50.781 (55.005)
Epoch: [4][550/573]	LR 0.099606	Time 0.399 (0.401)	Data 0.000 (0.001)	Loss 1.3237 (1.2730)	Prec@1 50.781 (55.244)
Epoch: [4][573/573]	LR 0.099606	Time 0.178 (0.401)	Data 0.000 (0.001)	Loss 1.3224 (1.2708)	Prec@1 51.220 (55.297)
****************************************
test_set:	 * Prec@1 78.023
****************************************
Epoch: [5][50/573]	LR 0.099384	Time 0.392 (0.406)	Data 0.000 (0.007)	Loss 1.3305 (1.1963)	Prec@1 54.688 (57.922)
Epoch: [5][100/573]	LR 0.099384	Time 0.404 (0.404)	Data 0.000 (0.004)	Loss 1.3366 (1.1896)	Prec@1 46.094 (58.141)
Epoch: [5][150/573]	LR 0.099384	Time 0.404 (0.404)	Data 0.000 (0.003)	Loss 1.1837 (1.1824)	Prec@1 57.031 (58.573)
Epoch: [5][200/573]	LR 0.099384	Time 0.401 (0.403)	Data 0.000 (0.002)	Loss 1.2065 (1.1772)	Prec@1 54.688 (58.742)
Epoch: [5][250/573]	LR 0.099384	Time 0.404 (0.402)	Data 0.000 (0.002)	Loss 1.1529 (1.1783)	Prec@1 60.156 (58.706)
Epoch: [5][300/573]	LR 0.099384	Time 0.404 (0.402)	Data 0.000 (0.001)	Loss 1.1871 (1.1781)	Prec@1 57.031 (58.620)
Epoch: [5][350/573]	LR 0.099384	Time 0.404 (0.403)	Data 0.000 (0.001)	Loss 1.1835 (1.1812)	Prec@1 59.375 (58.529)
Epoch: [5][400/573]	LR 0.099384	Time 0.401 (0.403)	Data 0.000 (0.001)	Loss 1.1499 (1.1763)	Prec@1 60.156 (58.684)
Epoch: [5][450/573]	LR 0.099384	Time 0.402 (0.402)	Data 0.000 (0.001)	Loss 1.2469 (1.1755)	Prec@1 57.812 (58.708)
Epoch: [5][500/573]	LR 0.099384	Time 0.401 (0.402)	Data 0.000 (0.001)	Loss 1.0223 (1.1751)	Prec@1 63.281 (58.752)
Epoch: [5][550/573]	LR 0.099384	Time 0.399 (0.402)	Data 0.000 (0.001)	Loss 1.2170 (1.1714)	Prec@1 56.250 (58.868)
Epoch: [5][573/573]	LR 0.099384	Time 0.181 (0.401)	Data 0.000 (0.001)	Loss 1.0746 (1.1707)	Prec@1 63.415 (58.949)
****************************************
test_set:	 * Prec@1 83.443
****************************************
Epoch: [6][50/573]	LR 0.099114	Time 0.385 (0.403)	Data 0.000 (0.004)	Loss 1.2818 (1.1495)	Prec@1 57.031 (59.938)
Epoch: [6][100/573]	LR 0.099114	Time 0.403 (0.402)	Data 0.000 (0.002)	Loss 1.1441 (1.1423)	Prec@1 58.594 (60.062)
Epoch: [6][150/573]	LR 0.099114	Time 0.404 (0.403)	Data 0.000 (0.002)	Loss 1.3154 (1.1425)	Prec@1 50.000 (59.990)
Epoch: [6][200/573]	LR 0.099114	Time 0.399 (0.402)	Data 0.000 (0.001)	Loss 1.2275 (1.1440)	Prec@1 53.125 (59.812)
Epoch: [6][250/573]	LR 0.099114	Time 0.403 (0.401)	Data 0.000 (0.001)	Loss 1.1732 (1.1403)	Prec@1 57.812 (60.025)
Epoch: [6][300/573]	LR 0.099114	Time 0.404 (0.401)	Data 0.000 (0.001)	Loss 1.1065 (1.1389)	Prec@1 65.625 (60.060)
Traceback (most recent call last):
  File "main.py", line 310, in <module>
    main()
  File "main.py", line 160, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "main.py", line 238, in train
    losses.update(loss.item(), input.size(0))
KeyboardInterrupt
