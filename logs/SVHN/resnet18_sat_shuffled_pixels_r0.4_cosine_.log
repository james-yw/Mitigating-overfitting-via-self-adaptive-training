Namespace(arch='resnet18', base_width=64, batch_size=128, data_root='../DATASETS/SVHN', dataset='SVHN', epochs=100, evaluate=False, loss='sat', lr=0.1, lr_gamma=0.1, lr_milestones=[40, 80], lr_schedule='cosine', momentum=0.9, noise_info=None, noise_rate=0.4, noise_type='shuffled_pixels', optimizer='sgd', print_freq=50, result_dir='results/SVHN/resnet18_sat_shuffled_pixels_r0.4_cosine_', resume='', sat_alpha=0.9, sat_es=60, save_dir='ckpts/SVHN/resnet18_sat_shuffled_pixels_r0.4_cosine_', save_freq=0, seed=3238, start_epoch=0, train_sets='trainval', turn_off_aug=False, use_refined_label=False, val_sets=['test_set'], weight_decay=0.0005, workers=4)
Using downloaded and verified file: ../DATASETS/SVHN/train_32x32.mat
data shape: (73257, 3, 32, 32)
Using downloaded and verified file: ../DATASETS/SVHN/test_32x32.mat
data shape: (26032, 3, 32, 32)
Randomizing 40.0 percent of images with `shuffled_pixels` scheme.
n_train:73257  n_rand:29302 randomize_indices:[63577 31588 55713 ... 40368 46301 19259]
Noise info is saved to ckpts/SVHN/resnet18_sat_shuffled_pixels_r0.4_cosine_
Size of dataset: 73257.
Using `SGD` optimizer
Using `cosine` schedule
****************************************
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch: [0][50/573]	LR 0.100000	Time 0.358 (0.435)	Data 0.000 (0.021)	Loss 2.2167 (2.9187)	Prec@1 23.438 (16.328)
Epoch: [0][100/573]	LR 0.100000	Time 0.360 (0.396)	Data 0.000 (0.010)	Loss 2.2565 (2.5823)	Prec@1 19.531 (17.656)
Epoch: [0][150/573]	LR 0.100000	Time 0.401 (0.389)	Data 0.001 (0.007)	Loss 2.2576 (2.4676)	Prec@1 17.969 (17.969)
Epoch: [0][200/573]	LR 0.100000	Time 0.401 (0.391)	Data 0.000 (0.005)	Loss 2.2950 (2.4108)	Prec@1 13.281 (18.270)
Epoch: [0][250/573]	LR 0.100000	Time 0.404 (0.393)	Data 0.000 (0.004)	Loss 2.2477 (2.3769)	Prec@1 19.531 (18.359)
Epoch: [0][300/573]	LR 0.100000	Time 0.391 (0.394)	Data 0.000 (0.004)	Loss 2.2641 (2.3541)	Prec@1 14.062 (18.443)
Epoch: [0][350/573]	LR 0.100000	Time 0.400 (0.395)	Data 0.000 (0.003)	Loss 2.2643 (2.3370)	Prec@1 14.062 (18.594)
Epoch: [0][400/573]	LR 0.100000	Time 0.404 (0.395)	Data 0.001 (0.003)	Loss 2.2205 (2.3248)	Prec@1 18.750 (18.529)
Epoch: [0][450/573]	LR 0.100000	Time 0.404 (0.396)	Data 0.000 (0.003)	Loss 2.1793 (2.3156)	Prec@1 27.344 (18.488)
Epoch: [0][500/573]	LR 0.100000	Time 0.403 (0.397)	Data 0.000 (0.002)	Loss 2.1967 (2.3080)	Prec@1 25.781 (18.575)
Epoch: [0][550/573]	LR 0.100000	Time 0.402 (0.397)	Data 0.001 (0.002)	Loss 2.1942 (2.3015)	Prec@1 21.875 (18.636)
Epoch: [0][573/573]	LR 0.100000	Time 3.466 (0.403)	Data 0.000 (0.002)	Loss 2.2614 (2.2991)	Prec@1 4.878 (18.647)
****************************************
test_set:	 * Prec@1 19.568
****************************************
Epoch: [1][50/573]	LR 0.099975	Time 0.400 (0.407)	Data 0.000 (0.007)	Loss 2.2397 (2.2387)	Prec@1 17.969 (19.094)
Epoch: [1][100/573]	LR 0.099975	Time 0.402 (0.398)	Data 0.000 (0.004)	Loss 2.3095 (2.2427)	Prec@1 16.406 (18.570)
Epoch: [1][150/573]	LR 0.099975	Time 0.401 (0.398)	Data 0.000 (0.003)	Loss 2.2009 (2.2372)	Prec@1 21.875 (18.906)
Epoch: [1][200/573]	LR 0.099975	Time 0.401 (0.398)	Data 0.000 (0.002)	Loss 2.2218 (2.2374)	Prec@1 21.094 (18.965)
Epoch: [1][250/573]	LR 0.099975	Time 0.403 (0.399)	Data 0.000 (0.002)	Loss 2.1987 (2.2364)	Prec@1 19.531 (18.931)
Epoch: [1][300/573]	LR 0.099975	Time 0.402 (0.399)	Data 0.000 (0.001)	Loss 2.1751 (2.2295)	Prec@1 25.000 (19.427)
Epoch: [1][350/573]	LR 0.099975	Time 0.399 (0.399)	Data 0.000 (0.001)	Loss 2.1265 (2.2126)	Prec@1 21.094 (20.163)
Epoch: [1][400/573]	LR 0.099975	Time 0.403 (0.399)	Data 0.000 (0.001)	Loss 2.0329 (2.1892)	Prec@1 26.562 (21.180)
Epoch: [1][450/573]	LR 0.099975	Time 0.403 (0.400)	Data 0.000 (0.001)	Loss 1.8040 (2.1609)	Prec@1 32.812 (22.283)
Epoch: [1][500/573]	LR 0.099975	Time 0.401 (0.400)	Data 0.000 (0.001)	Loss 1.8076 (2.1301)	Prec@1 35.938 (23.558)
Epoch: [1][550/573]	LR 0.099975	Time 0.402 (0.400)	Data 0.000 (0.001)	Loss 1.6253 (2.0973)	Prec@1 37.500 (24.820)
Epoch: [1][573/573]	LR 0.099975	Time 0.183 (0.400)	Data 0.000 (0.001)	Loss 1.9083 (2.0828)	Prec@1 36.585 (25.350)
****************************************
test_set:	 * Prec@1 39.402
****************************************
Epoch: [2][50/573]	LR 0.099901	Time 0.400 (0.406)	Data 0.000 (0.005)	Loss 1.6251 (1.6341)	Prec@1 35.938 (42.297)
Epoch: [2][100/573]	LR 0.099901	Time 0.401 (0.402)	Data 0.000 (0.003)	Loss 1.6142 (1.5599)	Prec@1 46.875 (45.477)
Epoch: [2][150/573]	LR 0.099901	Time 0.401 (0.401)	Data 0.000 (0.002)	Loss 1.3140 (1.4896)	Prec@1 56.250 (48.375)
Epoch: [2][200/573]	LR 0.099901	Time 0.402 (0.400)	Data 0.000 (0.002)	Loss 1.0455 (1.4313)	Prec@1 67.188 (50.547)
Epoch: [2][250/573]	LR 0.099901	Time 0.405 (0.401)	Data 0.000 (0.001)	Loss 1.1210 (1.3703)	Prec@1 63.281 (52.850)
Epoch: [2][300/573]	LR 0.099901	Time 0.402 (0.401)	Data 0.000 (0.001)	Loss 1.0656 (1.3157)	Prec@1 65.625 (54.766)
Epoch: [2][350/573]	LR 0.099901	Time 0.401 (0.401)	Data 0.000 (0.001)	Loss 1.0274 (1.2689)	Prec@1 68.750 (56.529)
Epoch: [2][400/573]	LR 0.099901	Time 0.404 (0.401)	Data 0.000 (0.001)	Loss 1.0236 (1.2278)	Prec@1 67.969 (57.992)
Epoch: [2][450/573]	LR 0.099901	Time 0.403 (0.401)	Data 0.000 (0.001)	Loss 0.8251 (1.1952)	Prec@1 73.438 (59.200)
Epoch: [2][500/573]	LR 0.099901	Time 0.402 (0.401)	Data 0.000 (0.001)	Loss 0.7858 (1.1632)	Prec@1 72.656 (60.377)
Epoch: [2][550/573]	LR 0.099901	Time 0.402 (0.401)	Data 0.000 (0.001)	Loss 0.8313 (1.1323)	Prec@1 73.438 (61.452)
Epoch: [2][573/573]	LR 0.099901	Time 0.181 (0.401)	Data 0.000 (0.001)	Loss 0.8958 (1.1193)	Prec@1 73.171 (61.901)
****************************************
test_set:	 * Prec@1 67.298
****************************************
Epoch: [3][50/573]	LR 0.099778	Time 0.400 (0.405)	Data 0.000 (0.005)	Loss 0.7433 (0.8007)	Prec@1 75.000 (74.125)
Epoch: [3][100/573]	LR 0.099778	Time 0.401 (0.401)	Data 0.000 (0.003)	Loss 0.8501 (0.7784)	Prec@1 72.656 (74.492)
Epoch: [3][150/573]	LR 0.099778	Time 0.402 (0.401)	Data 0.000 (0.002)	Loss 0.7287 (0.7757)	Prec@1 76.562 (74.641)
Epoch: [3][200/573]	LR 0.099778	Time 0.403 (0.400)	Data 0.000 (0.001)	Loss 0.7342 (0.7679)	Prec@1 70.312 (74.777)
Epoch: [3][250/573]	LR 0.099778	Time 0.404 (0.401)	Data 0.000 (0.001)	Loss 0.4590 (0.7598)	Prec@1 88.281 (75.062)
Epoch: [3][300/573]	LR 0.099778	Time 0.403 (0.401)	Data 0.000 (0.001)	Loss 0.7384 (0.7525)	Prec@1 71.875 (75.341)
Epoch: [3][350/573]	LR 0.099778	Time 0.401 (0.401)	Data 0.000 (0.001)	Loss 0.4930 (0.7416)	Prec@1 85.938 (75.770)
Epoch: [3][400/573]	LR 0.099778	Time 0.404 (0.401)	Data 0.000 (0.001)	Loss 0.7405 (0.7352)	Prec@1 75.000 (76.008)
Epoch: [3][450/573]	LR 0.099778	Time 0.404 (0.401)	Data 0.000 (0.001)	Loss 0.6196 (0.7283)	Prec@1 81.250 (76.262)
Epoch: [3][500/573]	LR 0.099778	Time 0.401 (0.401)	Data 0.000 (0.001)	Loss 0.6926 (0.7210)	Prec@1 76.562 (76.544)
Epoch: [3][550/573]	LR 0.099778	Time 0.402 (0.401)	Data 0.000 (0.001)	Loss 0.6168 (0.7138)	Prec@1 81.250 (76.793)
Epoch: [3][573/573]	LR 0.099778	Time 0.180 (0.401)	Data 0.000 (0.001)	Loss 0.4498 (0.7108)	Prec@1 87.805 (76.895)
****************************************
test_set:	 * Prec@1 86.470
****************************************
Epoch: [4][50/573]	LR 0.099606	Time 0.401 (0.405)	Data 0.000 (0.005)	Loss 0.5289 (0.6431)	Prec@1 84.375 (79.641)
Epoch: [4][100/573]	LR 0.099606	Time 0.403 (0.402)	Data 0.000 (0.003)	Loss 0.7499 (0.6285)	Prec@1 75.000 (79.922)
Epoch: [4][150/573]	LR 0.099606	Time 0.402 (0.401)	Data 0.000 (0.002)	Loss 0.5207 (0.6212)	Prec@1 84.375 (80.219)
Epoch: [4][200/573]	LR 0.099606	Time 0.402 (0.401)	Data 0.000 (0.002)	Loss 0.7817 (0.6208)	Prec@1 76.562 (80.152)
Epoch: [4][250/573]	LR 0.099606	Time 0.404 (0.401)	Data 0.000 (0.001)	Loss 0.5595 (0.6171)	Prec@1 82.812 (80.234)
Epoch: [4][300/573]	LR 0.099606	Time 0.382 (0.401)	Data 0.000 (0.001)	Loss 0.6981 (0.6116)	Prec@1 78.125 (80.474)
Epoch: [4][350/573]	LR 0.099606	Time 0.400 (0.401)	Data 0.000 (0.001)	Loss 0.7318 (0.6060)	Prec@1 75.000 (80.703)
Epoch: [4][400/573]	LR 0.099606	Time 0.403 (0.401)	Data 0.000 (0.001)	Loss 0.3592 (0.6046)	Prec@1 89.062 (80.723)
Epoch: [4][450/573]	LR 0.099606	Time 0.404 (0.401)	Data 0.000 (0.001)	Loss 0.6547 (0.6001)	Prec@1 78.906 (80.868)
Epoch: [4][500/573]	LR 0.099606	Time 0.401 (0.401)	Data 0.000 (0.001)	Loss 0.3954 (0.5982)	Prec@1 87.500 (80.939)
Epoch: [4][550/573]	LR 0.099606	Time 0.402 (0.401)	Data 0.000 (0.001)	Loss 0.5477 (0.5955)	Prec@1 82.812 (81.047)
Epoch: [4][573/573]	LR 0.099606	Time 0.182 (0.401)	Data 0.000 (0.001)	Loss 0.4414 (0.5948)	Prec@1 82.927 (81.079)
****************************************
test_set:	 * Prec@1 87.104
****************************************
Epoch: [5][50/573]	LR 0.099384	Time 0.401 (0.407)	Data 0.000 (0.007)	Loss 0.4848 (0.5401)	Prec@1 86.719 (82.359)
Epoch: [5][100/573]	LR 0.099384	Time 0.403 (0.402)	Data 0.000 (0.003)	Loss 0.5258 (0.5519)	Prec@1 77.344 (82.023)
Epoch: [5][150/573]	LR 0.099384	Time 0.401 (0.401)	Data 0.000 (0.002)	Loss 0.5774 (0.5540)	Prec@1 79.688 (82.161)
Epoch: [5][200/573]	LR 0.099384	Time 0.402 (0.401)	Data 0.000 (0.002)	Loss 0.5320 (0.5512)	Prec@1 85.156 (82.457)
Epoch: [5][250/573]	LR 0.099384	Time 0.404 (0.401)	Data 0.000 (0.002)	Loss 0.5531 (0.5518)	Prec@1 82.031 (82.503)
Epoch: [5][300/573]	LR 0.099384	Time 0.398 (0.401)	Data 0.000 (0.001)	Loss 0.5272 (0.5497)	Prec@1 83.594 (82.552)
Epoch: [5][350/573]	LR 0.099384	Time 0.400 (0.401)	Data 0.000 (0.001)	Loss 0.6725 (0.5487)	Prec@1 83.594 (82.605)
Epoch: [5][400/573]	LR 0.099384	Time 0.403 (0.401)	Data 0.000 (0.001)	Loss 0.6160 (0.5475)	Prec@1 78.125 (82.621)
Epoch: [5][450/573]	LR 0.099384	Time 0.404 (0.401)	Data 0.000 (0.001)	Loss 0.5020 (0.5450)	Prec@1 83.594 (82.675)
Epoch: [5][500/573]	LR 0.099384	Time 0.402 (0.402)	Data 0.000 (0.001)	Loss 0.4074 (0.5433)	Prec@1 87.500 (82.758)
Epoch: [5][550/573]	LR 0.099384	Time 0.402 (0.402)	Data 0.000 (0.001)	Loss 0.4995 (0.5404)	Prec@1 80.469 (82.865)
Epoch: [5][573/573]	LR 0.099384	Time 0.180 (0.401)	Data 0.000 (0.001)	Loss 0.3629 (0.5404)	Prec@1 92.683 (82.886)
****************************************
test_set:	 * Prec@1 85.641
****************************************
Epoch: [6][50/573]	LR 0.099114	Time 0.400 (0.406)	Data 0.000 (0.005)	Loss 0.5743 (0.5013)	Prec@1 81.250 (84.391)
Epoch: [6][100/573]	LR 0.099114	Time 0.402 (0.402)	Data 0.000 (0.003)	Loss 0.4071 (0.5177)	Prec@1 87.500 (83.562)
Epoch: [6][150/573]	LR 0.099114	Time 0.402 (0.401)	Data 0.000 (0.002)	Loss 0.4912 (0.5217)	Prec@1 81.250 (83.474)
Epoch: [6][200/573]	LR 0.099114	Time 0.403 (0.401)	Data 0.000 (0.002)	Loss 0.4763 (0.5195)	Prec@1 86.719 (83.555)
Epoch: [6][250/573]	LR 0.099114	Time 0.404 (0.401)	Data 0.000 (0.001)	Loss 0.5873 (0.5195)	Prec@1 82.031 (83.472)
Epoch: [6][300/573]	LR 0.099114	Time 0.400 (0.401)	Data 0.000 (0.001)	Loss 0.3807 (0.5176)	Prec@1 86.719 (83.589)
Epoch: [6][350/573]	LR 0.099114	Time 0.400 (0.401)	Data 0.000 (0.001)	Loss 0.4919 (0.5131)	Prec@1 85.156 (83.799)
Epoch: [6][400/573]	LR 0.099114	Time 0.403 (0.401)	Data 0.000 (0.001)	Loss 0.3335 (0.5108)	Prec@1 90.625 (83.912)
Epoch: [6][450/573]	LR 0.099114	Time 0.359 (0.400)	Data 0.000 (0.001)	Loss 0.4627 (0.5112)	Prec@1 83.594 (83.856)
Epoch: [6][500/573]	LR 0.099114	Time 0.354 (0.396)	Data 0.000 (0.001)	Loss 0.4935 (0.5103)	Prec@1 82.031 (83.903)
Epoch: [6][550/573]	LR 0.099114	Time 0.311 (0.388)	Data 0.000 (0.001)	Loss 0.4317 (0.5081)	Prec@1 86.719 (83.970)
Epoch: [6][573/573]	LR 0.099114	Time 0.143 (0.385)	Data 0.000 (0.001)	Loss 0.8087 (0.5070)	Prec@1 80.488 (84.004)
****************************************
test_set:	 * Prec@1 87.850
****************************************
Epoch: [7][50/573]	LR 0.098796	Time 0.284 (0.314)	Data 0.000 (0.004)	Loss 0.3790 (0.5003)	Prec@1 86.719 (84.391)
Epoch: [7][100/573]	LR 0.098796	Time 0.314 (0.312)	Data 0.000 (0.002)	Loss 0.4278 (0.5056)	Prec@1 85.938 (84.188)
Epoch: [7][150/573]	LR 0.098796	Time 0.311 (0.312)	Data 0.000 (0.002)	Loss 0.4323 (0.4983)	Prec@1 88.281 (84.406)
Epoch: [7][200/573]	LR 0.098796	Time 0.313 (0.311)	Data 0.000 (0.001)	Loss 0.5014 (0.4921)	Prec@1 82.031 (84.602)
Epoch: [7][250/573]	LR 0.098796	Time 0.314 (0.311)	Data 0.000 (0.001)	Loss 0.5939 (0.4890)	Prec@1 81.250 (84.628)
Epoch: [7][300/573]	LR 0.098796	Time 0.313 (0.312)	Data 0.000 (0.001)	Loss 0.4592 (0.4891)	Prec@1 86.719 (84.633)
Epoch: [7][350/573]	LR 0.098796	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.3991 (0.4878)	Prec@1 89.844 (84.719)
Epoch: [7][400/573]	LR 0.098796	Time 0.315 (0.312)	Data 0.000 (0.001)	Loss 0.4201 (0.4841)	Prec@1 89.062 (84.875)
Epoch: [7][450/573]	LR 0.098796	Time 0.314 (0.312)	Data 0.000 (0.001)	Loss 0.5805 (0.4835)	Prec@1 80.469 (84.844)
Epoch: [7][500/573]	LR 0.098796	Time 0.313 (0.312)	Data 0.000 (0.001)	Loss 0.4147 (0.4816)	Prec@1 85.938 (84.888)
Epoch: [7][550/573]	LR 0.098796	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.3852 (0.4806)	Prec@1 85.938 (84.952)
Epoch: [7][573/573]	LR 0.098796	Time 0.142 (0.312)	Data 0.000 (0.001)	Loss 0.6418 (0.4811)	Prec@1 87.805 (84.924)
****************************************
test_set:	 * Prec@1 86.989
****************************************
Epoch: [8][50/573]	LR 0.098429	Time 0.277 (0.314)	Data 0.000 (0.005)	Loss 0.3352 (0.5001)	Prec@1 89.062 (84.547)
Epoch: [8][100/573]	LR 0.098429	Time 0.314 (0.312)	Data 0.000 (0.002)	Loss 0.4621 (0.4801)	Prec@1 88.281 (85.070)
Epoch: [8][150/573]	LR 0.098429	Time 0.312 (0.312)	Data 0.000 (0.002)	Loss 0.3922 (0.4791)	Prec@1 84.375 (85.016)
Epoch: [8][200/573]	LR 0.098429	Time 0.313 (0.311)	Data 0.000 (0.001)	Loss 0.5184 (0.4783)	Prec@1 82.812 (85.016)
Epoch: [8][250/573]	LR 0.098429	Time 0.314 (0.311)	Data 0.000 (0.001)	Loss 0.6363 (0.4758)	Prec@1 78.125 (85.206)
Epoch: [8][300/573]	LR 0.098429	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.4311 (0.4766)	Prec@1 86.719 (85.148)
Epoch: [8][350/573]	LR 0.098429	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.4522 (0.4740)	Prec@1 90.625 (85.248)
Epoch: [8][400/573]	LR 0.098429	Time 0.314 (0.312)	Data 0.000 (0.001)	Loss 0.4163 (0.4721)	Prec@1 86.719 (85.318)
Epoch: [8][450/573]	LR 0.098429	Time 0.314 (0.312)	Data 0.000 (0.001)	Loss 0.3825 (0.4725)	Prec@1 88.281 (85.306)
Epoch: [8][500/573]	LR 0.098429	Time 0.311 (0.312)	Data 0.000 (0.001)	Loss 0.3154 (0.4709)	Prec@1 91.406 (85.380)
Epoch: [8][550/573]	LR 0.098429	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.6586 (0.4725)	Prec@1 79.688 (85.324)
Epoch: [8][573/573]	LR 0.098429	Time 0.139 (0.312)	Data 0.000 (0.001)	Loss 0.4285 (0.4724)	Prec@1 85.366 (85.331)
****************************************
test_set:	 * Prec@1 85.844
****************************************
Epoch: [9][50/573]	LR 0.098015	Time 0.283 (0.314)	Data 0.000 (0.005)	Loss 0.4825 (0.4764)	Prec@1 82.812 (84.969)
Epoch: [9][100/573]	LR 0.098015	Time 0.314 (0.312)	Data 0.000 (0.002)	Loss 0.6645 (0.4701)	Prec@1 82.031 (85.070)
Epoch: [9][150/573]	LR 0.098015	Time 0.312 (0.312)	Data 0.000 (0.002)	Loss 0.4339 (0.4614)	Prec@1 83.594 (85.531)
Epoch: [9][200/573]	LR 0.098015	Time 0.313 (0.311)	Data 0.000 (0.001)	Loss 0.5006 (0.4546)	Prec@1 85.938 (85.848)
Epoch: [9][250/573]	LR 0.098015	Time 0.314 (0.311)	Data 0.000 (0.001)	Loss 0.2947 (0.4547)	Prec@1 91.406 (85.841)
Epoch: [9][300/573]	LR 0.098015	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.3404 (0.4533)	Prec@1 89.844 (85.917)
Epoch: [9][350/573]	LR 0.098015	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.4614 (0.4539)	Prec@1 82.031 (85.946)
Epoch: [9][400/573]	LR 0.098015	Time 0.315 (0.312)	Data 0.000 (0.001)	Loss 0.4144 (0.4530)	Prec@1 88.281 (85.959)
Epoch: [9][450/573]	LR 0.098015	Time 0.313 (0.312)	Data 0.000 (0.001)	Loss 0.4817 (0.4514)	Prec@1 85.156 (86.016)
Epoch: [9][500/573]	LR 0.098015	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.5984 (0.4541)	Prec@1 81.250 (85.902)
Epoch: [9][550/573]	LR 0.098015	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.5062 (0.4524)	Prec@1 83.594 (85.939)
Epoch: [9][573/573]	LR 0.098015	Time 0.141 (0.312)	Data 0.000 (0.001)	Loss 0.5653 (0.4535)	Prec@1 82.927 (85.913)
****************************************
test_set:	 * Prec@1 83.766
****************************************
Epoch: [10][50/573]	LR 0.097553	Time 0.292 (0.314)	Data 0.000 (0.004)	Loss 0.3244 (0.4463)	Prec@1 90.625 (86.375)
Epoch: [10][100/573]	LR 0.097553	Time 0.314 (0.312)	Data 0.000 (0.002)	Loss 0.3873 (0.4455)	Prec@1 85.156 (86.148)
Epoch: [10][150/573]	LR 0.097553	Time 0.311 (0.312)	Data 0.000 (0.001)	Loss 0.4184 (0.4475)	Prec@1 88.281 (86.182)
Epoch: [10][200/573]	LR 0.097553	Time 0.312 (0.311)	Data 0.000 (0.001)	Loss 0.3876 (0.4505)	Prec@1 87.500 (86.012)
Epoch: [10][250/573]	LR 0.097553	Time 0.313 (0.311)	Data 0.000 (0.001)	Loss 0.4912 (0.4492)	Prec@1 84.375 (86.097)
Epoch: [10][300/573]	LR 0.097553	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.4777 (0.4483)	Prec@1 82.812 (86.039)
Epoch: [10][350/573]	LR 0.097553	Time 0.313 (0.312)	Data 0.000 (0.001)	Loss 0.3582 (0.4481)	Prec@1 89.062 (86.112)
Epoch: [10][400/573]	LR 0.097553	Time 0.313 (0.312)	Data 0.000 (0.001)	Loss 0.4718 (0.4468)	Prec@1 85.156 (86.141)
Epoch: [10][450/573]	LR 0.097553	Time 0.314 (0.312)	Data 0.000 (0.001)	Loss 0.3720 (0.4469)	Prec@1 89.062 (86.161)
Epoch: [10][500/573]	LR 0.097553	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.4147 (0.4454)	Prec@1 85.938 (86.198)
Epoch: [10][550/573]	LR 0.097553	Time 0.313 (0.312)	Data 0.000 (0.001)	Loss 0.4062 (0.4457)	Prec@1 86.719 (86.203)
Epoch: [10][573/573]	LR 0.097553	Time 0.141 (0.312)	Data 0.000 (0.001)	Loss 0.2417 (0.4449)	Prec@1 92.683 (86.240)
****************************************
test_set:	 * Prec@1 89.336
****************************************
Epoch: [11][50/573]	LR 0.097044	Time 0.274 (0.314)	Data 0.000 (0.004)	Loss 0.3569 (0.4461)	Prec@1 89.844 (86.406)
Epoch: [11][100/573]	LR 0.097044	Time 0.314 (0.312)	Data 0.000 (0.002)	Loss 0.3809 (0.4385)	Prec@1 88.281 (86.461)
Epoch: [11][150/573]	LR 0.097044	Time 0.312 (0.312)	Data 0.000 (0.002)	Loss 0.4514 (0.4335)	Prec@1 85.938 (86.443)
Epoch: [11][200/573]	LR 0.097044	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.3670 (0.4321)	Prec@1 87.500 (86.480)
Epoch: [11][250/573]	LR 0.097044	Time 0.314 (0.312)	Data 0.000 (0.001)	Loss 0.4260 (0.4321)	Prec@1 85.156 (86.444)
Epoch: [11][300/573]	LR 0.097044	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.3832 (0.4363)	Prec@1 87.500 (86.341)
Epoch: [11][350/573]	LR 0.097044	Time 0.313 (0.312)	Data 0.000 (0.001)	Loss 0.4726 (0.4350)	Prec@1 85.938 (86.359)
Epoch: [11][400/573]	LR 0.097044	Time 0.314 (0.312)	Data 0.000 (0.001)	Loss 0.6361 (0.4349)	Prec@1 82.031 (86.422)
Epoch: [11][450/573]	LR 0.097044	Time 0.313 (0.312)	Data 0.000 (0.001)	Loss 0.4949 (0.4305)	Prec@1 81.250 (86.557)
Epoch: [11][500/573]	LR 0.097044	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.4023 (0.4325)	Prec@1 83.594 (86.472)
Epoch: [11][550/573]	LR 0.097044	Time 0.311 (0.312)	Data 0.000 (0.001)	Loss 0.3349 (0.4324)	Prec@1 89.844 (86.480)
Epoch: [11][573/573]	LR 0.097044	Time 0.142 (0.312)	Data 0.000 (0.001)	Loss 0.3617 (0.4322)	Prec@1 85.366 (86.486)
****************************************
test_set:	 * Prec@1 91.453
****************************************
Epoch: [12][50/573]	LR 0.096489	Time 0.267 (0.313)	Data 0.000 (0.004)	Loss 0.3430 (0.4362)	Prec@1 91.406 (86.562)
Epoch: [12][100/573]	LR 0.096489	Time 0.314 (0.312)	Data 0.000 (0.002)	Loss 0.4204 (0.4217)	Prec@1 86.719 (86.812)
Epoch: [12][150/573]	LR 0.096489	Time 0.312 (0.312)	Data 0.000 (0.002)	Loss 0.5017 (0.4234)	Prec@1 85.938 (86.844)
Epoch: [12][200/573]	LR 0.096489	Time 0.313 (0.311)	Data 0.000 (0.001)	Loss 0.4867 (0.4251)	Prec@1 85.156 (86.793)
Epoch: [12][250/573]	LR 0.096489	Time 0.313 (0.311)	Data 0.000 (0.001)	Loss 0.4127 (0.4284)	Prec@1 89.062 (86.750)
Epoch: [12][300/573]	LR 0.096489	Time 0.313 (0.312)	Data 0.000 (0.001)	Loss 0.3737 (0.4288)	Prec@1 89.844 (86.693)
Epoch: [12][350/573]	LR 0.096489	Time 0.311 (0.312)	Data 0.000 (0.001)	Loss 0.5413 (0.4308)	Prec@1 85.156 (86.654)
Epoch: [12][400/573]	LR 0.096489	Time 0.314 (0.312)	Data 0.000 (0.001)	Loss 0.5549 (0.4266)	Prec@1 82.031 (86.781)
Epoch: [12][450/573]	LR 0.096489	Time 0.313 (0.312)	Data 0.000 (0.001)	Loss 0.6476 (0.4263)	Prec@1 80.469 (86.766)
Epoch: [12][500/573]	LR 0.096489	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.3571 (0.4272)	Prec@1 86.719 (86.781)
Epoch: [12][550/573]	LR 0.096489	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.3935 (0.4263)	Prec@1 89.062 (86.787)
Epoch: [12][573/573]	LR 0.096489	Time 0.141 (0.312)	Data 0.000 (0.001)	Loss 0.4141 (0.4260)	Prec@1 87.805 (86.793)
****************************************
test_set:	 * Prec@1 90.435
****************************************
Epoch: [13][50/573]	LR 0.095888	Time 0.267 (0.313)	Data 0.000 (0.004)	Loss 0.5206 (0.4152)	Prec@1 85.938 (87.516)
Epoch: [13][100/573]	LR 0.095888	Time 0.314 (0.312)	Data 0.000 (0.002)	Loss 0.5018 (0.4160)	Prec@1 82.812 (87.477)
Epoch: [13][150/573]	LR 0.095888	Time 0.312 (0.312)	Data 0.000 (0.002)	Loss 0.3322 (0.4157)	Prec@1 88.281 (87.339)
Epoch: [13][200/573]	LR 0.095888	Time 0.312 (0.311)	Data 0.000 (0.001)	Loss 0.3175 (0.4151)	Prec@1 91.406 (87.262)
Epoch: [13][250/573]	LR 0.095888	Time 0.314 (0.311)	Data 0.000 (0.001)	Loss 0.3364 (0.4148)	Prec@1 87.500 (87.206)
Epoch: [13][300/573]	LR 0.095888	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.5903 (0.4173)	Prec@1 83.594 (87.128)
Epoch: [13][350/573]	LR 0.095888	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.4041 (0.4200)	Prec@1 84.375 (87.080)
Epoch: [13][400/573]	LR 0.095888	Time 0.314 (0.312)	Data 0.000 (0.001)	Loss 0.4860 (0.4239)	Prec@1 83.594 (86.965)
Epoch: [13][450/573]	LR 0.095888	Time 0.315 (0.312)	Data 0.000 (0.001)	Loss 0.3505 (0.4206)	Prec@1 89.062 (87.033)
Epoch: [13][500/573]	LR 0.095888	Time 0.313 (0.312)	Data 0.000 (0.001)	Loss 0.4668 (0.4234)	Prec@1 87.500 (86.945)
Epoch: [13][550/573]	LR 0.095888	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.5026 (0.4230)	Prec@1 85.156 (86.932)
Epoch: [13][573/573]	LR 0.095888	Time 0.141 (0.312)	Data 0.000 (0.001)	Loss 0.6468 (0.4232)	Prec@1 80.488 (86.925)
****************************************
test_set:	 * Prec@1 88.668
****************************************
Epoch: [14][50/573]	LR 0.095241	Time 0.268 (0.313)	Data 0.000 (0.004)	Loss 0.5182 (0.4014)	Prec@1 82.812 (87.125)
Epoch: [14][100/573]	LR 0.095241	Time 0.314 (0.312)	Data 0.000 (0.002)	Loss 0.5330 (0.4127)	Prec@1 80.469 (87.164)
Epoch: [14][150/573]	LR 0.095241	Time 0.312 (0.312)	Data 0.000 (0.002)	Loss 0.4887 (0.4136)	Prec@1 85.938 (87.109)
Epoch: [14][200/573]	LR 0.095241	Time 0.313 (0.311)	Data 0.000 (0.001)	Loss 0.3980 (0.4108)	Prec@1 87.500 (87.242)
Epoch: [14][250/573]	LR 0.095241	Time 0.314 (0.311)	Data 0.000 (0.001)	Loss 0.4804 (0.4124)	Prec@1 84.375 (87.181)
Epoch: [14][300/573]	LR 0.095241	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.4371 (0.4125)	Prec@1 86.719 (87.245)
Epoch: [14][350/573]	LR 0.095241	Time 0.310 (0.312)	Data 0.000 (0.001)	Loss 0.3903 (0.4127)	Prec@1 86.719 (87.176)
Epoch: [14][400/573]	LR 0.095241	Time 0.314 (0.312)	Data 0.000 (0.001)	Loss 0.3181 (0.4116)	Prec@1 87.500 (87.246)
Epoch: [14][450/573]	LR 0.095241	Time 0.314 (0.312)	Data 0.000 (0.001)	Loss 0.5436 (0.4146)	Prec@1 82.812 (87.207)
Epoch: [14][500/573]	LR 0.095241	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.2932 (0.4134)	Prec@1 88.281 (87.222)
Epoch: [14][550/573]	LR 0.095241	Time 0.312 (0.312)	Data 0.000 (0.001)	Loss 0.4103 (0.4127)	Prec@1 88.281 (87.220)
Epoch: [14][573/573]	LR 0.095241	Time 0.142 (0.312)	Data 0.000 (0.001)	Loss 0.5064 (0.4132)	Prec@1 82.927 (87.204)
****************************************
test_set:	Traceback (most recent call last):
  File "main.py", line 310, in <module>
    main()
  File "main.py", line 167, in main
    prec1 = validate(val_loader, model)
  File "main.py", line 277, in validate
    for i, (input, target, _) in enumerate(val_loader):
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 435, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1068, in _next_data
    idx, data = self._get_data()
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1024, in _get_data
    success, data = self._try_get_data()
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 872, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/opt/conda/lib/python3.6/queue.py", line 173, in get
    self.not_empty.wait(remaining)
  File "/opt/conda/lib/python3.6/threading.py", line 299, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt
