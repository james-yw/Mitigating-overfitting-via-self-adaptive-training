Namespace(arch='resnet18', base_width=64, batch_size=128, data_root='../DATASETS/SVHN', dataset='SVHN', epochs=100, evaluate=False, loss='ce', lr=0.1, lr_gamma=0.1, lr_milestones=[40, 80], lr_schedule='cosine', momentum=0.9, noise_info=None, noise_rate=0.4, noise_type='corrupted_label', optimizer='sgd', print_freq=50, result_dir='results/SVHN/resnet18_ce_corrupted_label_r0.4_cosine_', resume='', sat_alpha=0.9, sat_es=0, save_dir='ckpts/SVHN/resnet18_ce_corrupted_label_r0.4_cosine_', save_freq=0, seed=4833, start_epoch=0, train_sets='trainval', turn_off_aug=False, use_refined_label=False, val_sets=['test_set'], weight_decay=0.0005, workers=4)
Using downloaded and verified file: ../DATASETS/SVHN/train_32x32.mat
data shape: (73257, 3, 32, 32)
Using downloaded and verified file: ../DATASETS/SVHN/test_32x32.mat
data shape: (26032, 3, 32, 32)
Randomizing 40.0 percent of labels 
Noisy labels saved to ckpts/SVHN/resnet18_ce_corrupted_label_r0.4_cosine_/noisy_idx_labels.npy
Size of dataset: 73257.
Label error rate: 0.36.
Using `SGD` optimizer
Using `cosine` schedule
****************************************
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch: [0][50/573]	LR 0.100000	Time 0.087 (0.109)	Data 0.000 (0.006)	Loss 2.3040 (2.9231)	Prec@1 14.062 (12.203)
Epoch: [0][100/573]	LR 0.100000	Time 0.090 (0.093)	Data 0.000 (0.003)	Loss 2.2891 (2.6082)	Prec@1 19.531 (13.344)
Epoch: [0][150/573]	LR 0.100000	Time 0.089 (0.092)	Data 0.000 (0.002)	Loss 2.2887 (2.5000)	Prec@1 18.750 (13.979)
Epoch: [0][200/573]	LR 0.100000	Time 0.090 (0.092)	Data 0.000 (0.002)	Loss 2.2677 (2.4457)	Prec@1 15.625 (14.297)
Epoch: [0][250/573]	LR 0.100000	Time 0.089 (0.091)	Data 0.000 (0.001)	Loss 2.3056 (2.4129)	Prec@1 10.938 (14.597)
Epoch: [0][300/573]	LR 0.100000	Time 0.090 (0.091)	Data 0.000 (0.001)	Loss 2.3038 (2.3903)	Prec@1 13.281 (14.828)
Epoch: [0][350/573]	LR 0.100000	Time 0.090 (0.091)	Data 0.000 (0.001)	Loss 2.2706 (2.3738)	Prec@1 16.406 (14.953)
Epoch: [0][400/573]	LR 0.100000	Time 0.091 (0.091)	Data 0.000 (0.001)	Loss 2.2745 (2.3615)	Prec@1 14.844 (15.031)
Epoch: [0][450/573]	LR 0.100000	Time 0.091 (0.091)	Data 0.000 (0.001)	Loss 2.2578 (2.3524)	Prec@1 17.188 (15.115)
Epoch: [0][500/573]	LR 0.100000	Time 0.092 (0.092)	Data 0.000 (0.003)	Loss 2.2588 (2.3449)	Prec@1 17.969 (15.202)
Epoch: [0][550/573]	LR 0.100000	Time 0.096 (0.092)	Data 0.000 (0.002)	Loss 2.2882 (2.3393)	Prec@1 16.406 (15.168)
Epoch: [0][573/573]	LR 0.100000	Time 0.945 (0.093)	Data 0.000 (0.002)	Loss 2.3036 (2.3368)	Prec@1 9.756 (15.171)
****************************************
test_set:	 * Prec@1 19.518
****************************************
Epoch: [1][50/573]	LR 0.099975	Time 0.133 (0.134)	Data 0.000 (0.004)	Loss 2.3170 (2.2794)	Prec@1 7.812 (15.391)
Epoch: [1][100/573]	LR 0.099975	Time 0.134 (0.130)	Data 0.000 (0.002)	Loss 2.2508 (2.2769)	Prec@1 20.312 (15.805)
Epoch: [1][150/573]	LR 0.099975	Time 0.133 (0.131)	Data 0.000 (0.001)	Loss 2.2771 (2.2791)	Prec@1 16.406 (15.505)
Epoch: [1][200/573]	LR 0.099975	Time 0.134 (0.132)	Data 0.000 (0.001)	Loss 2.2616 (2.2804)	Prec@1 18.750 (15.266)
Epoch: [1][250/573]	LR 0.099975	Time 0.134 (0.132)	Data 0.000 (0.001)	Loss 2.2113 (2.2799)	Prec@1 25.000 (15.291)
Epoch: [1][300/573]	LR 0.099975	Time 0.134 (0.133)	Data 0.000 (0.001)	Loss 2.2373 (2.2801)	Prec@1 23.438 (15.352)
Epoch: [1][350/573]	LR 0.099975	Time 0.134 (0.133)	Data 0.000 (0.001)	Loss 2.2635 (2.2799)	Prec@1 14.062 (15.406)
Epoch: [1][400/573]	LR 0.099975	Time 0.133 (0.133)	Data 0.000 (0.001)	Loss 2.3225 (2.2802)	Prec@1 10.938 (15.352)
Epoch: [1][450/573]	LR 0.099975	Time 0.133 (0.133)	Data 0.000 (0.001)	Loss 2.3022 (2.2804)	Prec@1 13.281 (15.332)
Epoch: [1][500/573]	LR 0.099975	Time 0.104 (0.133)	Data 0.000 (0.001)	Loss 2.3177 (2.2800)	Prec@1 11.719 (15.389)
Epoch: [1][550/573]	LR 0.099975	Time 0.132 (0.132)	Data 0.000 (0.001)	Loss 2.2757 (2.2798)	Prec@1 17.188 (15.436)
Epoch: [1][573/573]	LR 0.099975	Time 0.041 (0.132)	Data 0.000 (0.001)	Loss 2.2245 (2.2794)	Prec@1 19.512 (15.450)
****************************************
test_set:	 * Prec@1 19.603
****************************************
Epoch: [2][50/573]	LR 0.099901	Time 0.133 (0.135)	Data 0.000 (0.004)	Loss 2.2805 (2.2824)	Prec@1 13.281 (15.125)
Epoch: [2][100/573]	LR 0.099901	Time 0.135 (0.131)	Data 0.000 (0.002)	Loss 2.3373 (2.2804)	Prec@1 11.719 (14.984)
Epoch: [2][150/573]	LR 0.099901	Time 0.135 (0.132)	Data 0.000 (0.002)	Loss 2.2720 (2.2792)	Prec@1 15.625 (15.250)
Epoch: [2][200/573]	LR 0.099901	Time 0.134 (0.133)	Data 0.000 (0.001)	Loss 2.2894 (2.2794)	Prec@1 14.062 (15.094)
Epoch: [2][250/573]	LR 0.099901	Time 0.134 (0.133)	Data 0.000 (0.001)	Loss 2.2812 (2.2793)	Prec@1 14.062 (15.209)
Epoch: [2][300/573]	LR 0.099901	Time 0.134 (0.133)	Data 0.000 (0.001)	Loss 2.2900 (2.2788)	Prec@1 11.719 (15.156)
Epoch: [2][350/573]	LR 0.099901	Time 0.134 (0.133)	Data 0.000 (0.001)	Loss 2.2647 (2.2787)	Prec@1 15.625 (15.196)
Epoch: [2][400/573]	LR 0.099901	Time 0.135 (0.133)	Data 0.000 (0.001)	Loss 2.3283 (2.2780)	Prec@1 7.812 (15.268)
Epoch: [2][450/573]	LR 0.099901	Time 0.134 (0.134)	Data 0.000 (0.001)	Loss 2.2627 (2.2762)	Prec@1 12.500 (15.528)
Epoch: [2][500/573]	LR 0.099901	Time 0.135 (0.134)	Data 0.000 (0.001)	Loss 2.2413 (2.2733)	Prec@1 18.750 (15.828)
Epoch: [2][550/573]	LR 0.099901	Time 0.133 (0.133)	Data 0.000 (0.001)	Loss 2.0865 (2.2662)	Prec@1 23.438 (16.442)
Epoch: [2][573/573]	LR 0.099901	Time 0.046 (0.133)	Data 0.000 (0.001)	Loss 2.3066 (2.2617)	Prec@1 19.512 (16.738)
****************************************
test_set:	 * Prec@1 37.903
****************************************
Epoch: [3][50/573]	LR 0.099778	Time 0.132 (0.135)	Data 0.000 (0.004)	Loss 2.1540 (2.1250)	Prec@1 26.562 (25.812)
Epoch: [3][100/573]	LR 0.099778	Time 0.134 (0.132)	Data 0.000 (0.002)	Loss 2.1686 (2.0920)	Prec@1 26.562 (28.195)
Epoch: [3][150/573]	LR 0.099778	Time 0.134 (0.132)	Data 0.000 (0.001)	Loss 2.0808 (2.0712)	Prec@1 33.594 (29.917)
Epoch: [3][200/573]	LR 0.099778	Time 0.134 (0.133)	Data 0.000 (0.001)	Loss 2.0779 (2.0471)	Prec@1 28.906 (31.258)
Epoch: [3][250/573]	LR 0.099778	Time 0.134 (0.133)	Data 0.000 (0.001)	Loss 1.8585 (2.0185)	Prec@1 37.500 (33.028)
Epoch: [3][300/573]	LR 0.099778	Time 0.135 (0.133)	Data 0.000 (0.001)	Loss 1.7155 (1.9936)	Prec@1 46.875 (34.625)
Epoch: [3][350/573]	LR 0.099778	Time 0.135 (0.134)	Data 0.000 (0.001)	Loss 1.8377 (1.9642)	Prec@1 48.438 (36.429)
Epoch: [3][400/573]	LR 0.099778	Time 0.135 (0.134)	Data 0.000 (0.001)	Loss 1.9873 (1.9429)	Prec@1 39.062 (37.891)
Epoch: [3][450/573]	LR 0.099778	Time 0.135 (0.134)	Data 0.000 (0.001)	Loss 1.7306 (1.9276)	Prec@1 53.906 (38.884)
Epoch: [3][500/573]	LR 0.099778	Time 0.134 (0.134)	Data 0.000 (0.001)	Loss 1.8234 (1.9096)	Prec@1 45.312 (40.006)
Epoch: [3][550/573]	LR 0.099778	Time 0.133 (0.134)	Data 0.000 (0.001)	Loss 1.8368 (1.8931)	Prec@1 46.094 (41.016)
Epoch: [3][573/573]	LR 0.099778	Time 0.060 (0.133)	Data 0.000 (0.001)	Loss 1.3288 (1.8859)	Prec@1 68.293 (41.436)
****************************************
test_set:	 * Prec@1 76.813
****************************************
Epoch: [4][50/573]	LR 0.099606	Time 0.133 (0.134)	Data 0.000 (0.003)	Loss 1.6376 (1.7145)	Prec@1 53.906 (51.359)
Epoch: [4][100/573]	LR 0.099606	Time 0.134 (0.132)	Data 0.000 (0.002)	Loss 1.6415 (1.7150)	Prec@1 57.812 (51.812)
Epoch: [4][150/573]	LR 0.099606	Time 0.134 (0.132)	Data 0.000 (0.001)	Loss 1.7639 (1.7115)	Prec@1 52.344 (51.906)
Epoch: [4][200/573]	LR 0.099606	Time 0.134 (0.133)	Data 0.000 (0.001)	Loss 1.6683 (1.7031)	Prec@1 51.562 (52.211)
Epoch: [4][250/573]	LR 0.099606	Time 0.134 (0.133)	Data 0.000 (0.001)	Loss 1.7371 (1.6976)	Prec@1 49.219 (52.378)
Epoch: [4][300/573]	LR 0.099606	Time 0.133 (0.133)	Data 0.000 (0.001)	Loss 1.6548 (1.6914)	Prec@1 57.031 (52.672)
Epoch: [4][350/573]	LR 0.099606	Time 0.134 (0.133)	Data 0.000 (0.001)	Loss 1.6145 (1.6897)	Prec@1 53.125 (52.770)
Epoch: [4][400/573]	LR 0.099606	Time 0.134 (0.133)	Data 0.000 (0.001)	Loss 1.5906 (1.6872)	Prec@1 58.594 (52.910)
Epoch: [4][450/573]	LR 0.099606	Time 0.133 (0.133)	Data 0.000 (0.001)	Loss 1.4553 (1.6851)	Prec@1 64.062 (52.960)
Epoch: [4][500/573]	LR 0.099606	Time 0.134 (0.134)	Data 0.000 (0.001)	Loss 1.5830 (1.6833)	Prec@1 58.594 (53.080)
Epoch: [4][550/573]	LR 0.099606	Time 0.133 (0.133)	Data 0.000 (0.000)	Loss 1.7462 (1.6825)	Prec@1 52.344 (53.159)
Epoch: [4][573/573]	LR 0.099606	Time 0.059 (0.133)	Data 0.000 (0.000)	Loss 1.7929 (1.6808)	Prec@1 51.220 (53.248)
****************************************
test_set:	 * Prec@1 81.696
****************************************
Epoch: [5][50/573]	LR 0.099384	Time 0.133 (0.135)	Data 0.000 (0.004)	Loss 1.7526 (1.6544)	Prec@1 51.562 (54.828)
Epoch: [5][100/573]	LR 0.099384	Time 0.134 (0.132)	Data 0.000 (0.002)	Loss 1.5832 (1.6460)	Prec@1 58.594 (55.133)
